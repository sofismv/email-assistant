# Постановка задачи

Разработка локального инструмента генерации персонализированных писем на основе кратких инструкций от пользователя.
Цель — сэкономить время на написание писем, сохраняя стиль общения пользователя.
Все данные являются чувствительными, передача за пределы локальной системы запрещена.

# Формат данных

**Вход:** Инструкция для написания письма
**Выход:** Письмо в стиле пользователя

# Метрики

- **MAUVE** — оценка сходства стиля
- **BLEU** — измерение качества генерации текста

# Валидация

- 10% обучающей выборки используется как validation set

# Основная модель

- **Модель:** [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- **Метод:** Finetune с использованием QLoRA (4-bit)
- **Данные:** персональный архив писем одного пользователя

**Формат датасета:**

- Инструкция (саммари письма)
- Ответ — полный текст письма
- Саммари генерируются автоматически базовой LLM

# Setup

```bash
conda create --name=email_assistant python=3.11.7
conda activate email_assistant
poetry install
```

# Train

## Системные требования

- **GPU:** NVIDIA A100 (40GB)
- **GPU RAM:**
  - Обучение: ~15 GB (с QLoRA)
  - Инференс: ~9 GB

## Установка и запуск

```bash
cd src/email_assistant
CUDA_VISIBLE_DEVICES=$DEVICE poetry run python train.py
```

## Этапы обучения:

- Загрузка архива писем (data.py)
- Генерация инструкций (summarize.py)
- Деление на train/test (split_data.py)
- Finetune модели с использованием LoRA (адаптер сохраняется отдельно)

# Production preparation
Финальная модель состоит из:

- **Базовой LLM** (Meta-Llama-3-8B-Instruct)
- **Адаптера LoRA** (adapter_model.safetensors)

Полная модель не сохраняется, только адаптер.

# Infer

```bash
CUDA_VISIBLE_DEVICES=$DEVICE poetry run python infer.py
```

## Описание

- Базовая модель объединяется с адаптером
- **Вход:** `.txt` файл с инструкциями (по одной на строку) или ввод вручную через CLI
- **Выход:** `.csv` файл с двумя столбцами:
  - `instruction`
  - `generated_email`
